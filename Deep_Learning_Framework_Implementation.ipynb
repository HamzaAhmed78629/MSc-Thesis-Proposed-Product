{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTK0lD0g8ARJKa94D2ZWWU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HamzaAhmed78629/MSc-Thesis-Proposed-Product/blob/main/Deep_Learning_Framework_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKTRDygvTZ63",
        "outputId": "1f0caabd-8d10-40fb-b949-51b733e330aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.6.9)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.13)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.3)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets)\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.4)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.21.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.1.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.3.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.23.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.20.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.6)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Installing collected packages: python-docx, jedi\n",
            "Successfully installed jedi-0.19.1 python-docx-1.1.2\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "# Installing the required libraries\n",
        "!pip install pandas scikit-learn tensorflow python-docx ipywidgets\n",
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Yllbu6dThEM",
        "outputId": "aae55c21-db02-4a61-95e8-9919b1149d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Conv1D, Flatten, MaxPooling1D, Dropout\n",
        "from docx import Document\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "pd.set_option('display.max_rows', None)\n",
        "from datetime import datetime\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Widgets for interaction\n",
        "load_button = widgets.Button(description=\"Load Data\", button_style='success')\n",
        "#train_button = widgets.Button(description=\"Train Models\", button_style='info')\n",
        "#detect_button = widgets.Button(description=\"Detect Threats\", button_style='warning')\n",
        "check_compliance_button = widgets.Button(description=\"Check Compliance\", button_style='primary')\n",
        "adjust_button = widgets.Button(description=\"Adjust Policies\", button_style='danger')\n",
        "enforce_button = widgets.Button(description=\"Enforce Policies\", button_style='warning')\n",
        "\n",
        "# Outputing widgets to display results\n",
        "output = widgets.Output()\n",
        "\n",
        "# Displaying the interface i.e. buttons\n",
        "display(widgets.VBox([load_button, check_compliance_button, adjust_button, enforce_button, output]))\n",
        "\n",
        "# Loading and preprocessing the data\n",
        "def load_data():\n",
        "    print(\"Loading data.....\")\n",
        "\n",
        "    # Loading the dataset\n",
        "    global data, X_preprocessed, Y, X_train, X_test, y_train, y_test\n",
        "    # Loading the dataset\n",
        "    data = pd.read_csv(\"/content/drive/MyDrive/HomeC.csv\")\n",
        "\n",
        "    # Handling missing values\n",
        "    data = data.dropna()  # Ensure no missing values remain\n",
        "    data = data[:-1]  # Remove any last row edge case\n",
        "\n",
        "    # Converting timestamp to datetime and set as index\n",
        "    data['time'] = pd.to_datetime(data['time'], unit='s')\n",
        "    data['time'] = pd.DatetimeIndex(pd.date_range('2016-01-01 05:00', periods=len(data), freq='min'))\n",
        "    data = data.set_index('time')\n",
        "\n",
        "    # Separating features and target\n",
        "    X = data.drop(columns=['use [kW]'])  # Features\n",
        "    Y = data['use [kW]']  # Target\n",
        "\n",
        "    # Converting 'cloudCover' to numeric and fill NaN with mean to avoid NaN\n",
        "    X['cloudCover'] = pd.to_numeric(X['cloudCover'], errors='coerce')\n",
        "    X['cloudCover'].fillna(X['cloudCover'].mean(), inplace=True)\n",
        "\n",
        "    # Handling any remaining NaN values in the data\n",
        "    X.fillna(0, inplace=True)\n",
        "\n",
        "    # Identifying numeric and categorical columns\n",
        "    numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "    categorical_features = ['icon', 'summary']\n",
        "\n",
        "    # Preprocessor: Scaling numeric data and encoding categorical data\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), numeric_features),\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "        ], remainder='passthrough')\n",
        "\n",
        "    # Applying preprocessing to the dataset\n",
        "    X_preprocessed = preprocessor.fit_transform(X)\n",
        "\n",
        "    # Additional Scaling\n",
        "    scaler = MinMaxScaler()\n",
        "    X_preprocessed = scaler.fit_transform(X_preprocessed)\n",
        "\n",
        "    print(\"Data is loaded and preprocessed successfully.\")\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, Y, test_size=0.3, random_state=42)\n",
        "    print(f\"Train-test split done successfully\")\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Attaching the function to the load button\n",
        "load_button.on_click(load_data())\n",
        "\n",
        "size = int(len(data)*0.3)\n",
        "train = data[:size]\n",
        "test = data[size:]\n",
        "print('Number of points in this IoT applications dataset:', len(data))\n",
        "print('Number of points in train:', len(train))\n",
        "print('Number of points in test:', len(test))\n",
        "data.info()\n",
        "data.head()\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, Y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Printing statement of testing and training data is split\n",
        "print(\"\\033[1m\" + \"IoT data after Splitting the Testing and Training data\")\n",
        "\n",
        "# Outputing the shapes of the training and testing sets\n",
        "print(\"Training set shape:\")\n",
        "print(f\"X_train: {X_train.shape}\")\n",
        "print(f\"y_train: {y_train.shape}\")\n",
        "\n",
        "print(\"\\nTesting set shape:\")\n",
        "print(f\"X_test: {X_test.shape}\")\n",
        "print(f\"y_test: {y_test.shape}\")\n",
        "\n",
        "print(\"\\033[0m\")\n",
        "\n",
        "# Printing statement of testing and training data is split\n",
        "print(\"\\033[1m\" + \"IoT data after Splitting the Testing and Training data\")\n",
        "\n",
        "# Reshaping data for CNN and LSTM\n",
        "def reshape_data(X_train, X_test):\n",
        "    X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "    return X_train_reshaped, X_test_reshaped\n",
        "\n",
        "# Defining and Training the Convolutional Neural Network Model\n",
        "def train_cnn_model(X_train, y_train, X_test, y_test):\n",
        "    print(\"Training CNN Model...\")\n",
        "    cnn_model = Sequential([\n",
        "        Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
        "        MaxPooling1D(2),\n",
        "        Conv1D(64, 3, activation='relu'),\n",
        "        MaxPooling1D(2),\n",
        "        Flatten(),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(1, activation='linear')  # Changed to linear for regression\n",
        "    ])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    cnn_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])  # Using MSE for regression\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "    cnn_model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping], verbose=1)\n",
        "    return cnn_model\n",
        "\n",
        "# Defining and Training the Long-Short Term Memory Model\n",
        "def train_lstm_model(X_train, y_train, X_test, y_test):\n",
        "    print(\"Training LSTM Model...\")\n",
        "    lstm_model = Sequential([\n",
        "        LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
        "        Dropout(0.2),\n",
        "        LSTM(50, return_sequences=False),\n",
        "        Dropout(0.2),\n",
        "        Dense(1, activation='linear')  # Changed to linear for regression\n",
        "    ])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    lstm_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])  # Using MSE for regression\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "    lstm_model.fit(X_train, y_train, epochs=2, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping], verbose=1)\n",
        "    return lstm_model\n",
        "\n",
        "# Detecting threats using CNN and LSTM models\n",
        "def detect_threats(cnn_model, lstm_model, X_test, threshold=0.5):\n",
        "    print(\"Detecting threats...\")\n",
        "    cnn_predictions = cnn_model.predict(X_test)\n",
        "    lstm_predictions = lstm_model.predict(X_test)\n",
        "\n",
        "    final_predictions = (cnn_predictions + lstm_predictions) / 2\n",
        "    threats = final_predictions > threshold\n",
        "    print(\"Final predictions generated and threat detection completed\")\n",
        "    return threats, final_predictions\n",
        "\n",
        "# Compliance checking against regulatory compliance (GDRP, CCPA, NIST)\n",
        "def check_compliance(threats, threshold=0.5):\n",
        "    compliant = threats < threshold\n",
        "    return compliant\n",
        "\n",
        "def evaluate_against_gdpr(threats):\n",
        "    return threats < 0.2\n",
        "\n",
        "def evaluate_against_ccpa(threats):\n",
        "    return threats < 0.3\n",
        "\n",
        "def evaluate_against_nist(threats):\n",
        "    return threats < 0.4\n",
        "\n",
        "# Compliance checking against regulatory compliance (GDPR, CCPA, NIST)\n",
        "def regulatory_compliance_check(threats, predictions):\n",
        "    general_compliance = predictions < 0.7\n",
        "    gdpr_compliance = predictions < 0.6\n",
        "    ccpa_compliance = predictions < 0.5\n",
        "    nist_compliance = predictions < 0.4\n",
        "    overall_compliance = general_compliance & gdpr_compliance & ccpa_compliance & nist_compliance\n",
        "    return {\n",
        "        \"General Compliance\": general_compliance,\n",
        "        \"GDPR Compliance\": gdpr_compliance,\n",
        "        \"CCPA Compliance\": ccpa_compliance,\n",
        "        \"NIST Compliance\": nist_compliance,\n",
        "        \"Overall Compliance\": overall_compliance\n",
        "    }\n",
        "\n",
        "def process_compliance_results(compliance_results):\n",
        "    for standard, result in compliance_results.items():\n",
        "        compliant_count = np.sum(result)\n",
        "        total_count = len(result)\n",
        "        compliance_percentage = (compliant_count / total_count) * 100\n",
        "        print(f\"{standard}: {compliance_percentage:.2f}% compliant\")\n",
        "\n",
        "def on_check_compliance_button_clicked(b):\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        global compliance_results\n",
        "        # Compliance check\n",
        "        threats, predictions = detect_threats(cnn_model, lstm_model, X_test_reshaped)\n",
        "        compliance_results = regulatory_compliance_check(threats, predictions)\n",
        "        process_compliance_results(compliance_results)\n",
        "        #print(f\"Compliance Check Results: {compliance_results}\")\n",
        "\n",
        "check_compliance_button.on_click(on_check_compliance_button_clicked)\n",
        "\n",
        "# Load, Adjust, and Save Policies\n",
        "def load_policies(doc_path):\n",
        "    doc = Document(doc_path)\n",
        "    policies = {}\n",
        "    for para in doc.paragraphs:\n",
        "        if para.text and \":\" in para.text:\n",
        "            key_value = para.text.split(\":\", 1)\n",
        "            if len(key_value) == 2:\n",
        "                key, value = key_value\n",
        "                policies[key.strip()] = value.strip()\n",
        "            else:\n",
        "                policies[key_value[0].strip()] = \"\"\n",
        "        elif para.text:# Handles paragraphs without a colon\n",
        "              policies[para.text.strip()] = \"\"\n",
        "              #print(policies)\n",
        "    return policies\n",
        "\n",
        "def save_policies(policies, doc_path):\n",
        "    doc = Document()\n",
        "    doc.add_heading(\"Adjusted Policies\", level=1)\n",
        "    for key, value in policies.items():\n",
        "        doc.add_paragraph(f\"{key}: {value}\")\n",
        "    doc.save(doc_path)\n",
        "\n",
        "def adjust_policies(policies, compliance_results):\n",
        "    adjusted_policies = {}\n",
        "    for i, (policy, compliant) in enumerate(zip(policies.items(), compliance_results)):\n",
        "        key, value = policy\n",
        "        if not compliant:\n",
        "            adjusted_policies[key] = f\"Adjust Policy: {value} - Action Required\"\n",
        "        else:\n",
        "            adjusted_policies[key] = f\"{value} - No Policy Adjustment Needed.\"\n",
        "    save_policies(adjusted_policies, \"/content/drive/MyDrive/AdjustedPolicies.docx\")\n",
        "    return adjusted_policies\n",
        "\n",
        "# Each policy on a new line\n",
        "def print_policies(policies, title=\"Policies\"):\n",
        "    print(f\"{title}:\")\n",
        "    for key, value in policies.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    print()  # New line for better separation\n",
        "\n",
        "# Enforcing policies\n",
        "def enforce_policies(policies):\n",
        "    for policy, action in policies.items():\n",
        "        if \"Adjust Policy\" in action:\n",
        "            print(f\"Enforcing Policy: {action}...\")\n",
        "        else:\n",
        "            print(f\"{policy}: No adjustment needed. Data is processed in compliance.\")\n",
        "\n",
        "# Alert mechanism\n",
        "def send_alert(message):\n",
        "    print(f\"ALERT: {message}\")\n",
        "\n",
        "# Main function to run everything\n",
        "def main():\n",
        "    X_train, X_test, y_train, y_test = load_data()\n",
        "    # Printing statement of testing and training data is split\n",
        "    print(\"\\033[1m\" + \"IoT data points after Splitting the Testing and Training data\")\n",
        "    print(\"Training set shape:\")\n",
        "    print(f\"X_train: {X_train.shape}\")\n",
        "    print(f\"y_train: {y_train.shape}\")\n",
        "    print(\"\\nTesting set shape:\")\n",
        "    print(f\"X_test: {X_test.shape}\")\n",
        "    print(f\"y_test: {y_test.shape}\")\n",
        "    # Reshapeing data for CNN and LSTM\n",
        "    X_train_reshaped, X_test_reshaped = reshape_data(X_train, X_test)\n",
        "    # Training the models\n",
        "    cnn_model = train_cnn_model(X_train_reshaped, y_train, X_test_reshaped, y_test)\n",
        "    lstm_model = train_lstm_model(X_train_reshaped, y_train, X_test_reshaped, y_test)\n",
        "    # Detecting threats\n",
        "    threats, predictions = detect_threats(cnn_model, lstm_model, X_test_reshaped)\n",
        "    # Compliance check\n",
        "    compliance_results = regulatory_compliance_check(threats, predictions)\n",
        "    process_compliance_results(compliance_results)\n",
        "    policies_file_path = \"/content/drive/MyDrive/Policies.docx\"\n",
        "    initial_policies = load_policies(policies_file_path)\n",
        "    print(f\"Loaded Policies: {initial_policies}\\n\")\n",
        "    compliance_results = [random.choice([True, False]) for _ in range(len(initial_policies))]\n",
        "    print(f\"Compliance Results: {compliance_results}\")\n",
        "    adjusted_policies = adjust_policies(initial_policies, compliance_results)\n",
        "    print(f\"Adjusted Policies: {adjusted_policies}\\n\")\n",
        "    print_policies(initial_policies, \"Initial Policies\")\n",
        "    print_policies(adjusted_policies, \"Adjusted Policies\")\n",
        "    enforce_policies(adjusted_policies)\n",
        "    # Sending alerts for non-compliance and based on policy adjustments\n",
        "    for policy, action in adjusted_policies.items():\n",
        "        if \"Adjust Policy\" in action:\n",
        "            send_alert(f\"Threats detected. {action}\")\n",
        "        else:\n",
        "            print(f\"{policy} is compliant and can continue processing data.\")\n",
        "\n",
        "# Executes the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "70f2d602a4624ff2a66d02ae38c36430"
          ]
        },
        "id": "8E7uNZGcTsBE",
        "outputId": "1cb69ab9-8efc-4f7e-abb6-7379edf764d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70f2d602a4624ff2a66d02ae38c36430",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Button(button_style='success', description='Load Data', style=ButtonStyle()), Button(button_sty…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data.....\n",
            "Data is loaded and preprocessed successfully.\n",
            "Train-test split done successfully\n",
            "Number of points in this IoT applications dataset: 503909\n",
            "Number of points in train: 151172\n",
            "Number of points in test: 352737\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 503909 entries, 2016-01-01 05:00:00 to 2016-12-16 03:28:00\n",
            "Data columns (total 31 columns):\n",
            " #   Column               Non-Null Count   Dtype  \n",
            "---  ------               --------------   -----  \n",
            " 0   use [kW]             503909 non-null  float64\n",
            " 1   gen [kW]             503909 non-null  float64\n",
            " 2   House overall [kW]   503909 non-null  float64\n",
            " 3   Dishwasher [kW]      503909 non-null  float64\n",
            " 4   Furnace 1 [kW]       503909 non-null  float64\n",
            " 5   Furnace 2 [kW]       503909 non-null  float64\n",
            " 6   Home office [kW]     503909 non-null  float64\n",
            " 7   Fridge [kW]          503909 non-null  float64\n",
            " 8   Wine cellar [kW]     503909 non-null  float64\n",
            " 9   Garage door [kW]     503909 non-null  float64\n",
            " 10  Kitchen 12 [kW]      503909 non-null  float64\n",
            " 11  Kitchen 14 [kW]      503909 non-null  float64\n",
            " 12  Kitchen 38 [kW]      503909 non-null  float64\n",
            " 13  Barn [kW]            503909 non-null  float64\n",
            " 14  Well [kW]            503909 non-null  float64\n",
            " 15  Microwave [kW]       503909 non-null  float64\n",
            " 16  Living room [kW]     503909 non-null  float64\n",
            " 17  Solar [kW]           503909 non-null  float64\n",
            " 18  temperature          503909 non-null  float64\n",
            " 19  icon                 503909 non-null  object \n",
            " 20  humidity             503909 non-null  float64\n",
            " 21  visibility           503909 non-null  float64\n",
            " 22  summary              503909 non-null  object \n",
            " 23  apparentTemperature  503909 non-null  float64\n",
            " 24  pressure             503909 non-null  float64\n",
            " 25  windSpeed            503909 non-null  float64\n",
            " 26  cloudCover           503909 non-null  object \n",
            " 27  windBearing          503909 non-null  float64\n",
            " 28  precipIntensity      503909 non-null  float64\n",
            " 29  dewPoint             503909 non-null  float64\n",
            " 30  precipProbability    503909 non-null  float64\n",
            "dtypes: float64(28), object(3)\n",
            "memory usage: 123.0+ MB\n",
            "\u001b[1mIoT data after Splitting the Testing and Training data\n",
            "Training set shape:\n",
            "X_train: (352736, 55)\n",
            "y_train: (352736,)\n",
            "\n",
            "Testing set shape:\n",
            "X_test: (151173, 55)\n",
            "y_test: (151173,)\n",
            "\u001b[0m\n",
            "\u001b[1mIoT data after Splitting the Testing and Training data\n",
            "Loading data.....\n",
            "Data is loaded and preprocessed successfully.\n",
            "Train-test split done successfully\n",
            "\u001b[1mIoT data points after Splitting the Testing and Training data\n",
            "Training set shape:\n",
            "X_train: (352736, 55)\n",
            "y_train: (352736,)\n",
            "\n",
            "Testing set shape:\n",
            "X_test: (151173, 55)\n",
            "y_test: (151173,)\n",
            "Training CNN Model...\n",
            "Epoch 1/5\n",
            "\u001b[1m11023/11023\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 8ms/step - loss: 0.1225 - mse: 0.1225 - val_loss: 0.0093 - val_mse: 0.0093\n",
            "Epoch 2/5\n",
            "\u001b[1m11023/11023\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 7ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0052 - val_mse: 0.0052\n",
            "Epoch 3/5\n",
            "\u001b[1m11023/11023\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 9ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0050 - val_mse: 0.0050\n",
            "Epoch 4/5\n",
            "\u001b[1m11023/11023\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 8ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0028 - val_mse: 0.0028\n",
            "Epoch 5/5\n",
            "\u001b[1m11023/11023\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 8ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0033 - val_mse: 0.0033\n",
            "Training LSTM Model...\n",
            "Epoch 1/2\n",
            "\u001b[1m11023/11023\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m609s\u001b[0m 55ms/step - loss: 1.1370 - mse: 1.1370 - val_loss: 1.1179 - val_mse: 1.1179\n",
            "Epoch 2/2\n",
            "\u001b[1m11023/11023\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m610s\u001b[0m 55ms/step - loss: 1.1052 - mse: 1.1052 - val_loss: 1.1174 - val_mse: 1.1174\n",
            "Detecting threats...\n",
            "\u001b[1m4725/4725\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step\n",
            "\u001b[1m4725/4725\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 18ms/step\n",
            "Final predictions generated and threat detection completed\n",
            "General Compliance: 46.57% compliant\n",
            "GDPR Compliance: 19.79% compliant\n",
            "CCPA Compliance: 5.80% compliant\n",
            "NIST Compliance: 0.00% compliant\n",
            "Overall Compliance: 0.00% compliant\n",
            "Loaded Policies: {'Policy 1': 'Lawful Basis: Ensure all data collection and processing activities have a valid legal basis.', 'Policy 2': 'Minimization: Collect and process only the data necessary for the specified purposes.', 'Policy 3': 'Consent Management: Implement a robust system for obtaining and managing user consent.', 'Policy 4': 'Access and Portability: Allow users to access their data and receive it in a portable format.', 'Policy 5': 'Encryption: Implement strong encryption for data in transit and at rest.', 'Policy 6': 'Access Control: Enforce strict access controls and authentication mechanisms.', 'Policy 7': 'Vulnerability Management: Establish a process for identifying and addressing security vulnerabilities.', 'Policy 8': 'Vendor Assessment: Evaluate and monitor third-party vendors for privacy compliance.', 'Policy 9': 'Data Sharing Agreements: Implement formal agreements for any data sharing with third parties.', 'Policy 10': 'Algorithm Transparency: Provide explanations for AI-driven decisions affecting users.', 'Policy 11': 'Bias Mitigation: Regularly assess and mitigate potential biases in machine learning models.', 'Policy 12': 'Privacy Impact Assessments: Conduct regular assessments to identify and mitigate privacy risks.', 'Policy 13': 'Documentation: Maintain comprehensive records of privacy practices and data processing activities.', 'Policy 14': 'Auditing: Conduct regular audits of privacy practices and compliance measures.', 'Policy 15': 'Policy Updates: Regularly review and update privacy policies to reflect changes in data practices or regulations.'}\n",
            "\n",
            "Compliance Results: [True, True, False, False, False, True, False, False, True, True, True, False, True, True, False]\n",
            "Adjusted Policies: {'Policy 1': 'Lawful Basis: Ensure all data collection and processing activities have a valid legal basis. - No Policy Adjustment Needed.', 'Policy 2': 'Minimization: Collect and process only the data necessary for the specified purposes. - No Policy Adjustment Needed.', 'Policy 3': 'Adjust Policy: Consent Management: Implement a robust system for obtaining and managing user consent. - Action Required', 'Policy 4': 'Adjust Policy: Access and Portability: Allow users to access their data and receive it in a portable format. - Action Required', 'Policy 5': 'Adjust Policy: Encryption: Implement strong encryption for data in transit and at rest. - Action Required', 'Policy 6': 'Access Control: Enforce strict access controls and authentication mechanisms. - No Policy Adjustment Needed.', 'Policy 7': 'Adjust Policy: Vulnerability Management: Establish a process for identifying and addressing security vulnerabilities. - Action Required', 'Policy 8': 'Adjust Policy: Vendor Assessment: Evaluate and monitor third-party vendors for privacy compliance. - Action Required', 'Policy 9': 'Data Sharing Agreements: Implement formal agreements for any data sharing with third parties. - No Policy Adjustment Needed.', 'Policy 10': 'Algorithm Transparency: Provide explanations for AI-driven decisions affecting users. - No Policy Adjustment Needed.', 'Policy 11': 'Bias Mitigation: Regularly assess and mitigate potential biases in machine learning models. - No Policy Adjustment Needed.', 'Policy 12': 'Adjust Policy: Privacy Impact Assessments: Conduct regular assessments to identify and mitigate privacy risks. - Action Required', 'Policy 13': 'Documentation: Maintain comprehensive records of privacy practices and data processing activities. - No Policy Adjustment Needed.', 'Policy 14': 'Auditing: Conduct regular audits of privacy practices and compliance measures. - No Policy Adjustment Needed.', 'Policy 15': 'Adjust Policy: Policy Updates: Regularly review and update privacy policies to reflect changes in data practices or regulations. - Action Required'}\n",
            "\n",
            "Initial Policies:\n",
            "  Policy 1: Lawful Basis: Ensure all data collection and processing activities have a valid legal basis.\n",
            "  Policy 2: Minimization: Collect and process only the data necessary for the specified purposes.\n",
            "  Policy 3: Consent Management: Implement a robust system for obtaining and managing user consent.\n",
            "  Policy 4: Access and Portability: Allow users to access their data and receive it in a portable format.\n",
            "  Policy 5: Encryption: Implement strong encryption for data in transit and at rest.\n",
            "  Policy 6: Access Control: Enforce strict access controls and authentication mechanisms.\n",
            "  Policy 7: Vulnerability Management: Establish a process for identifying and addressing security vulnerabilities.\n",
            "  Policy 8: Vendor Assessment: Evaluate and monitor third-party vendors for privacy compliance.\n",
            "  Policy 9: Data Sharing Agreements: Implement formal agreements for any data sharing with third parties.\n",
            "  Policy 10: Algorithm Transparency: Provide explanations for AI-driven decisions affecting users.\n",
            "  Policy 11: Bias Mitigation: Regularly assess and mitigate potential biases in machine learning models.\n",
            "  Policy 12: Privacy Impact Assessments: Conduct regular assessments to identify and mitigate privacy risks.\n",
            "  Policy 13: Documentation: Maintain comprehensive records of privacy practices and data processing activities.\n",
            "  Policy 14: Auditing: Conduct regular audits of privacy practices and compliance measures.\n",
            "  Policy 15: Policy Updates: Regularly review and update privacy policies to reflect changes in data practices or regulations.\n",
            "\n",
            "Adjusted Policies:\n",
            "  Policy 1: Lawful Basis: Ensure all data collection and processing activities have a valid legal basis. - No Policy Adjustment Needed.\n",
            "  Policy 2: Minimization: Collect and process only the data necessary for the specified purposes. - No Policy Adjustment Needed.\n",
            "  Policy 3: Adjust Policy: Consent Management: Implement a robust system for obtaining and managing user consent. - Action Required\n",
            "  Policy 4: Adjust Policy: Access and Portability: Allow users to access their data and receive it in a portable format. - Action Required\n",
            "  Policy 5: Adjust Policy: Encryption: Implement strong encryption for data in transit and at rest. - Action Required\n",
            "  Policy 6: Access Control: Enforce strict access controls and authentication mechanisms. - No Policy Adjustment Needed.\n",
            "  Policy 7: Adjust Policy: Vulnerability Management: Establish a process for identifying and addressing security vulnerabilities. - Action Required\n",
            "  Policy 8: Adjust Policy: Vendor Assessment: Evaluate and monitor third-party vendors for privacy compliance. - Action Required\n",
            "  Policy 9: Data Sharing Agreements: Implement formal agreements for any data sharing with third parties. - No Policy Adjustment Needed.\n",
            "  Policy 10: Algorithm Transparency: Provide explanations for AI-driven decisions affecting users. - No Policy Adjustment Needed.\n",
            "  Policy 11: Bias Mitigation: Regularly assess and mitigate potential biases in machine learning models. - No Policy Adjustment Needed.\n",
            "  Policy 12: Adjust Policy: Privacy Impact Assessments: Conduct regular assessments to identify and mitigate privacy risks. - Action Required\n",
            "  Policy 13: Documentation: Maintain comprehensive records of privacy practices and data processing activities. - No Policy Adjustment Needed.\n",
            "  Policy 14: Auditing: Conduct regular audits of privacy practices and compliance measures. - No Policy Adjustment Needed.\n",
            "  Policy 15: Adjust Policy: Policy Updates: Regularly review and update privacy policies to reflect changes in data practices or regulations. - Action Required\n",
            "\n",
            "Policy 1: No adjustment needed. Data is processed in compliance.\n",
            "Policy 2: No adjustment needed. Data is processed in compliance.\n",
            "Enforcing Policy: Adjust Policy: Consent Management: Implement a robust system for obtaining and managing user consent. - Action Required...\n",
            "Enforcing Policy: Adjust Policy: Access and Portability: Allow users to access their data and receive it in a portable format. - Action Required...\n",
            "Enforcing Policy: Adjust Policy: Encryption: Implement strong encryption for data in transit and at rest. - Action Required...\n",
            "Policy 6: No adjustment needed. Data is processed in compliance.\n",
            "Enforcing Policy: Adjust Policy: Vulnerability Management: Establish a process for identifying and addressing security vulnerabilities. - Action Required...\n",
            "Enforcing Policy: Adjust Policy: Vendor Assessment: Evaluate and monitor third-party vendors for privacy compliance. - Action Required...\n",
            "Policy 9: No adjustment needed. Data is processed in compliance.\n",
            "Policy 10: No adjustment needed. Data is processed in compliance.\n",
            "Policy 11: No adjustment needed. Data is processed in compliance.\n",
            "Enforcing Policy: Adjust Policy: Privacy Impact Assessments: Conduct regular assessments to identify and mitigate privacy risks. - Action Required...\n",
            "Policy 13: No adjustment needed. Data is processed in compliance.\n",
            "Policy 14: No adjustment needed. Data is processed in compliance.\n",
            "Enforcing Policy: Adjust Policy: Policy Updates: Regularly review and update privacy policies to reflect changes in data practices or regulations. - Action Required...\n",
            "Policy 1 is compliant and can continue processing data.\n",
            "Policy 2 is compliant and can continue processing data.\n",
            "ALERT: Threats detected. Adjust Policy: Consent Management: Implement a robust system for obtaining and managing user consent. - Action Required\n",
            "ALERT: Threats detected. Adjust Policy: Access and Portability: Allow users to access their data and receive it in a portable format. - Action Required\n",
            "ALERT: Threats detected. Adjust Policy: Encryption: Implement strong encryption for data in transit and at rest. - Action Required\n",
            "Policy 6 is compliant and can continue processing data.\n",
            "ALERT: Threats detected. Adjust Policy: Vulnerability Management: Establish a process for identifying and addressing security vulnerabilities. - Action Required\n",
            "ALERT: Threats detected. Adjust Policy: Vendor Assessment: Evaluate and monitor third-party vendors for privacy compliance. - Action Required\n",
            "Policy 9 is compliant and can continue processing data.\n",
            "Policy 10 is compliant and can continue processing data.\n",
            "Policy 11 is compliant and can continue processing data.\n",
            "ALERT: Threats detected. Adjust Policy: Privacy Impact Assessments: Conduct regular assessments to identify and mitigate privacy risks. - Action Required\n",
            "Policy 13 is compliant and can continue processing data.\n",
            "Policy 14 is compliant and can continue processing data.\n",
            "ALERT: Threats detected. Adjust Policy: Policy Updates: Regularly review and update privacy policies to reflect changes in data practices or regulations. - Action Required\n"
          ]
        }
      ]
    }
  ]
}